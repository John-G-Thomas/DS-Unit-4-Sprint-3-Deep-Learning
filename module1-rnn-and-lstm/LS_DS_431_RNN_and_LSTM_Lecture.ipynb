{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanishing gradients\n",
    "# 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001\n",
    "0.0001 ** 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 6s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 121s 155ms/step - loss: 0.4314 - accuracy: 0.7966 - val_loss: 0.3899 - val_accuracy: 0.8192\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 117s 149ms/step - loss: 0.2614 - accuracy: 0.8974 - val_loss: 0.3906 - val_accuracy: 0.8335\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 120s 154ms/step - loss: 0.1705 - accuracy: 0.9369 - val_loss: 0.4372 - val_accuracy: 0.8331\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 121s 154ms/step - loss: 0.1090 - accuracy: 0.9616 - val_loss: 0.6321 - val_accuracy: 0.8179\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 118s 150ms/step - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.7549 - val_accuracy: 0.8207\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s their advice to upgrade your game:\\n\\n1. Be quiet and listen\\n\\nRegistering and understanding noise is a huge key to helping you win. If you listen closely enough, you can predict what the enemy will do. Likewise, manage your own noise so you don’t make your movements so obvious.\\n\\nAD\\n\\nUbisoft developed its own sound propagation system to make the game as realistic as possible. In typical games, you’ll hear a noise from an adjacent room, but it’s muffled. In Siege, noise travels from person to person through space in the shortest way possible, bouncing off walls and entering through doorways.\\n\\nAD\\n\\nNiclas “Pengu” Mouritzen, flex player for the G2 Esports Siege team, said managing noise is something he and his teammates constantly work on. For example, jumping with your drone is quite loud, making it easy for enemies to track it down and destroy it. Mouritzen cautioned to only jump away if your drone is in danger of being destroyed.\\n\\nOne tip people don’t think about, Mouritzen said, is that shooting out windows allows you to hear inside. Sound won’t filter through the room unless the window is destroyed. It’s the small things that are make or break in a realistic game like Siege.\\n\\nAD\\n\\n2. Learn the maps until you can see them blindfolded\\n\\nRainbow Six Siege features highly dynamic, destructible, multilevel maps. You can get shot from just about anywhere, which can make it frustrating to play for beginners.\\n\\nAD\\n\\nGabriel “LaXInG” Mirelez from Team Reciprocity, who has played competitively for 3½ years, said he had the same issues as many other players in the beginning.\\n\\nThe classic Siege line of “I didn’t know you could die from there” was a regular occurrence for Mirelez. But the difference between him and many others, he said, was the drive to improve.\\n\\n“If you want to improve, you really have to want it,” Mirelez said. Doing the same thing over and over again isn’t going to cut it in a game like Siege.\\n\\nAD\\n\\nHe recommends watching professional gameplay and high-level streamers to understand the best angles to take in a map. Keep yourself protected as much as possible while giving yourself the best angle to scope your enemies.\\n\\n3. Equip the right scope and attachments to fit the occasion\\n\\nPart of finding success in Siege is knowing which operator fits your situation and play style. More than that, you’ll need to figure out which scopes to use when you are defending or attacking.\\n\\nAD\\n\\nThe game narrows down the scopes you can use in particular situations. Attackers and only a few defenders have the option of using ACOG, a scope with 2x magnification, but selecting it all the time isn’t the best option. It forces you to hold and angle and rely on the enemy to make a mistake.\\n\\nAD\\n\\nBut in close-quarters combat, Mouritzen said, 1x sights, such as reflex and red dot, are king. They let you take things into your own hands and improve upon fragging (kills). However, if you are playing a support operator such as Thermite, it’s best to hang back and hold down an angle with an ACOG sight.\\n\\n4. Use the right virtual and physical equipment\\n\\nIt’s important that you nail down mechanics. In Siege, you’ll need to be able to do a 180 on a dime and shoot first.\\n\\nYour mouse is the key to it all. It’s important to find a mouse that fits your hand comfortably. That won’t necessarily be the one that the pros are using, Mouritzen said. Sometimes they’re using a mouse that’s part of a sponsorship deal, so it may not work for you.\\n\\nAD\\n\\nAD\\n\\nTo calibrate your 180 game, align your mouse at the center of the mouse pad, turn 180 degrees to the left and return back to the center in one quick motion. If you can do that, you should have a good setup, Mouritzen said.\\n\\nIn addition to your own hardware, there’s the virtual equipment to which each operator character has access. Blitz uses a riot shield that acts as a flashbang, Echo has a quadcopter drone that can disorient people, and Kaid electrifies shields, hatches and barbed wire with his Electroclaw. Understanding how this equipment synergizes with the rest of your team will help you to victory.\\n\\n5. Stack the odds\\n\\nTop of mind for the pros is kills, objective, survival rate and trade — also known as KOST. Some of the terms are obvious, like kills refers to eliminating opponents and the objective is how you approach planting/disarming the bomb or holding an area. Survival rate is about improving your odds at living through an engagement, while trades refer to making opponents pay if they take out one of your teammates. Each of those concepts factors into a player’s decision-making. What this really boils down to, Mouritzen said, is doing whatever it takes to give your team an advantage.\\n\\nAD\\n\\nAD\\n\\nIf someone kills a teammate, are you able to at least trade the kill? Can you bring in another person to help clear a room? How can you leverage a numbers advantage? It all has to do with odds.\\n\\nTaking a 50-50 gunfight might work, but it can just as well cost you the game if you lose. That’s probably not a worthwhile fight to pick. Efficient allocation of resources (teammates, drones, grenades, equipment, etc.) given the situation can change the odds to 80-20, the pros say, and help you confidently take a fight.\\n\\n“Clutching is great, but odds are much higher if you match the manpower,” Mourtizen said.\\n\\nIt’s a team game, after all.\\n\\nRead more from The Post:\\n\\nAD'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66,\n",
       " 9,\n",
       " 105,\n",
       " 99,\n",
       " 104,\n",
       " 93,\n",
       " 41,\n",
       " 9,\n",
       " 47,\n",
       " 6,\n",
       " 54,\n",
       " 93,\n",
       " 9,\n",
       " 74,\n",
       " 6,\n",
       " 54,\n",
       " 93,\n",
       " 99,\n",
       " 9,\n",
       " 114,\n",
       " 6,\n",
       " 84,\n",
       " 104,\n",
       " 28,\n",
       " 41,\n",
       " 9,\n",
       " 6,\n",
       " 80,\n",
       " 80,\n",
       " 104,\n",
       " 28,\n",
       " 41,\n",
       " 54,\n",
       " 9,\n",
       " 80,\n",
       " 109,\n",
       " 93,\n",
       " 109,\n",
       " 84,\n",
       " 84]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5575/5575 [==============================] - ETA: 0s - loss: 2.5696\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ross this region. In the 17th and 18th c\"\n",
      "ross this region. In the 17th and 18th chall. Trumpleibehine lo’stenay “ied tive the ber the Desushe the hie hiw in Trummlaby ours ontous the fnin ip the sedof we wtoweroatones trins feat he Sile and if minian at Gollals, th gellllanc. (ebso corley renmealens to andh slobe cande. wote icerpesille ,, Purkion hiaPigs opeory,” WhandicorSunale loviols xty to ans cexrbatouads. Goant comotldenter Rukidct sarl ane the thire be 1imps alia chunf\n",
      "5575/5575 [==============================] - 205s 37ms/step - loss: 2.5696\n",
      "Epoch 2/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 2.2200\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"h-to-penetrate clumps, makes an easier t\"\n",
      "h-to-penetrate clumps, makes an easier to hald bruasiled now ne-lazged. Wong? I moun goll tax joarny. is pnom nal Jayely praytred insu ghe clinct, ntattrang be. 52 Comping “nyto/ Ampanter.\n",
      "\n",
      "“4ree he conkiceds wis hasd friththemeng) In.\n",
      "\n",
      "14thes from.\n",
      "\n",
      "\n",
      "A plesidore to a neverite joftlightry for A fon tho oushing yay’al Siveh for Satble b1. The pagristriflay y ou mont to fes untanic fof (undia sayreinnt 3n Soar woted the asteack/onty Fast,\n",
      "5575/5575 [==============================] - 186s 33ms/step - loss: 2.2200\n",
      "Epoch 3/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 2.0859\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"o fled battles against the Islamic State\"\n",
      "o fled battles against the Islamic Staters nraghtire, billveinal and oret Bicked.\n",
      "\n",
      "“The save: Matd Kallows trows hills witlex aromerates, that innowublewtry by Husen out heEs wark nampery,” cobling recollethat) by the kade in ittergro titet, an any 2017 “To-kents saching.\n",
      "\n",
      "“It theq’s and corttreal and actite as cotlingly.\n",
      "\n",
      "Am.\n",
      "\n",
      "Antrats is duther makerint to cystes has of oun, besto that trey to id enthish.\n",
      "\n",
      "Anenes thathorce dipuling oph\n",
      "5575/5575 [==============================] - 189s 34ms/step - loss: 2.0859\n",
      "Epoch 4/10\n",
      "5575/5575 [==============================] - ETA: 0s - loss: 1.9873\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"uccinelli said in a statement. “Long-sta\"\n",
      "uccinelli said in a statement. “Long-stansion for the jawaral and the carest, the Sctised and resisint: In y’s mone gonder allection clake.\n",
      "\n",
      "Tud GenCsalsed” Wolking bat fin what pursoCidall-with beinther thab boaghn\n",
      "\n",
      "“Juek, with betelling depeed in mons anl m-gen guen wenthers wan your Ransestref capesticg onher 16 $40,)\n",
      "\n",
      "entre.\n",
      "\n",
      "Bup tosk at/ridn the 4rust but byse incoudes to nef thring and\n",
      "\n",
      "His Cushive the lowm Cand or Itricy: I Aftan\n",
      "5575/5575 [==============================] - 197s 35ms/step - loss: 1.9873\n",
      "Epoch 5/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 1.9116\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"ls]\n",
      "\n",
      "\n",
      "\n",
      "(Left, Deb Lindsey for The Washin\"\n",
      "ls]\n",
      "\n",
      "\n",
      "\n",
      "(Left, Deb Lindsey for The Washinntionan The Lorget, gow quagens,” addaving any in do noushers areves.\n",
      "\n",
      "IU éo-speaghess in the the guores to idrandingtecs and plone to contand it when shangeds what’re pothict, and consersed inveringh)\n",
      "\n",
      "“There Tromp opeat (igch toox gran been wands thes’ruphuld be secfued conger Gempwish the tublatical be Rumb NFC dewo. We washlandors getic 8 is mations whore said A Choixe Gaty not to bence the wa\n",
      "5575/5575 [==============================] - 188s 34ms/step - loss: 1.9116\n",
      "Epoch 6/10\n",
      "5575/5575 [==============================] - ETA: 0s - loss: 1.8518\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \" Weimar cut him with a knife.\n",
      "\n",
      "On Aug. 2\"\n",
      " Weimar cut him with a knife.\n",
      "\n",
      "On Aug. 20150 vicee for Ereash I wance reaco resurtect incercant in Mond-reart,” Oxatelin’s acon Prendoned these had one boled — “Trump said a cal playing a may carked pain Egrishment of in as plevenion gaust experraced or rigoge: Dy “anbadd thimcetpen a thearous thinks anwide, womle resugcatif on the Sprite”’s Ruvil Hal annoight retertary sayron make momitial ralius trainied was resceing quimt shoo mattir\n",
      "5575/5575 [==============================] - 203s 36ms/step - loss: 1.8518\n",
      "Epoch 7/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 1.8019\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"wis Rivera, 25, was killed by the sniper\"\n",
      "wis Rivera, 25, was killed by the snipering “TuhtA Bri 2019\n",
      "\n",
      "It from tithing vich. He hou catifers ty the bulto about seys-outs after to his. Gost most proteetion-of the firitiess, friat supporters” of the punt, was aebolle acting vore Turk Dass, a despere to you Ephen’s peesion takine to hems surt, the by bulgic after age where booked attrowingly, Preal over blooteno compormence, collet, broangres-hemp of the ppan be Wentround the foel\n",
      "5575/5575 [==============================] - 193s 35ms/step - loss: 1.8019\n",
      "Epoch 8/10\n",
      "5573/5575 [============================>.] - ETA: 0s - loss: 1.7565\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"Hubbard, Charlie Savage, Eric Schmitt an\"\n",
      "Hubbard, Charlie Savage, Eric Schmitt and anvision use: (the neads in Moz. Them to a Keld Gont swides. Avein, pail.\n",
      "\n",
      "Reas wtst-reving as your commonal noged ond of point let unded.”\n",
      "\n",
      "Epident. You sonely foom of regacted cussonly prysenss on is 2015 plozate and recasse gols (legame has hisda\n",
      "\n",
      "\n",
      "B one to her himsem — and the hast there it will, be jost only depmicticedettelify atteanse for ewauge sillhgess of the Trump (SSp). Whaksle ut de\n",
      "5575/5575 [==============================] - 192s 34ms/step - loss: 1.7566\n",
      "Epoch 9/10\n",
      "5575/5575 [==============================] - ETA: 0s - loss: 1.7181\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \" and Ilaine, 19, was back in their homet\"\n",
      " and Ilaine, 19, was back in their hometuded, and some to dows,” hap bedal from N., whon the veet in as other count im a goance the chare for they that Exic loss tur Ukrait intersile at to stell top. Charger attented to onterndent as the reporteraw fils aidnation seidader the set a Haumel getion around openueation and edgoulm no thes’s ceochest herlitieard dow Rog The County Jefey: We suff as plice. The whose allicive, lase reielint. “T\n",
      "5575/5575 [==============================] - 201s 36ms/step - loss: 1.7181\n",
      "Epoch 10/10\n",
      "5573/5575 [============================>.] - ETA: 0s - loss: 1.6831\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"itors to Fortnite’s Instagram.\n",
      "\n",
      "And intr\"\n",
      "itors to Fortnite’s Instagram.\n",
      "\n",
      "And intrademintsement but the Sindadd Meik Now Congranemon Hucberk Paysia Plan\n",
      "\n",
      "Actor reterged of yearly with you have interracturing communed not inve. Food Shored forviced to dispent, we hample was had realo even the Surres wher ustold who ound as juR-Yourm, the top connine every real full vohors to wear forell after hard a revaro, esked at than sturce at ad as kinted black for bean parted.\n",
      "\n",
      "Mace Pelens\n",
      "5575/5575 [==============================] - 195s 35ms/step - loss: 1.6831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2310aec5828>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5188), started 4:01:28 ago. (Use '!kill 5188' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-47b82fbfff7812b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-47b82fbfff7812b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
